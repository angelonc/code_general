%
%function [InfoData]=infwordrasterperiodicpanzeri(RASTER,B,Fm,T)
%
%   FILE NAME       : INF WORD RASTER PERIODIC PANZERI
%   DESCRIPTION     : Entropy & Noise Entropy of a periodic Spike Train 
%                     obtained from the rastergram by computing the 
%                     Probability Distribution, P(W|t,s), of finding a B 
%                     letter Word, W, in the Spike Train at time T for a
%                     given periodic stimulus, s.
%
%   RASTER          : Rastergram
%	B               : Length of Word, number of bits per cycle for
%                     generating P(W) and P(W,t)
%   Fm              : Sound modulation Frequency (Hz)
%   T               : Amount of time to remove at begingin of raster
%                     (Default==0)
%
%Returned Variables
%
%   InfoData        : Data structure containing all mutual information
%                     results
%                     .HWordt   : Noise Entropy per Word
%                     .HSect    : Noise Entropy per Second
%                     .HSpiket  : Noise Entropy per Spike
%                     .HWord    : Entropy per Word
%                     .HSec     : Entropy per Second
%                     .HSpike   : Entropy per Spike
%                     .Rate     : Mean Spike Rate
%                     .W        : Coded words for entropy calculation
%                     .Wt       : Coded words for noise entropy calculation
%                     .P        : Word distribution function
%                     .Pt       : Word distribution function for noise entropy
%                     .dt       : Actual Temporal Resolution Used
%
% (C) Monty A. Escabi, Aug. 2008
%
function [InfoData]=infwordrasterperiodicpanzeri(RASTER,B,Fm,T)

%Input Args
if nargin<4
    T=0;    
end

%Temporal Resolution
Fsd=Fm*B;
dt=1/Fsd;
Fs=RASTER(1).Fs;
L=max(round(Fs/Fsd),1);

%Finding the Possible Maximum Number of spikes per bin (changed Aug 2012)
Max=-9999;
for k=1:B
    [RASTERc]=raster2cyclerastermatrix(RASTER,Fm,1,T,Fsd,(k-1)/Fsd);
    Max=max(Max,max(max(RASTERc))/Fsd);
end
%Max=-9999;
%for k=1:length(RASTER)
%    Max=ceil(max([Max , 1/Fsd ./ ( diff(RASTER(k).spet)/Fs )]));    %Theoretical max number of spikes given observed ISIs
%end

%Binary Mask
D=Max+1;
Mask=D.^(0:B-1);

%Generating Cycle Matrix desired resolution and zero cycle phase
[RASTERc]=raster2cyclerastermatrix(RASTER,Fm,1,T,Fsd,0);
RASTERc=sparse(RASTERc)/Fsd;    %Normalizes amplitude and makes sparse
RASTERt=RASTERc(2:end,:);
RASTERc=RASTERc(1:end-1,:);

%Computing Word Distribution for Noise Entropy: P(W,t)
Ntrials=length(RASTER);
Ncycles=floor((RASTER(1).T-ceil(T*Fm)/Fm)/(1/Fm));  %Note that ceil(T*Fm)/Fm is the amount of time to remove at begining of raster
Wt=zeros(B,Ntrials*Ncycles);
Pt=[];
for k=1:B %This loop is used to change the phase value so that all phases are considered

%    %Generating Matrix From Raster at desired resolution and desired phase
%    [RASTERc]=raster2cyclerastermatrix(RASTER,Fm,1,T,Fsd,(k-1)/Fsd);
%    RASTERc=sparse(RASTERc)/Fsd;    %Normalizes amplitude and makes sparse

    %Circularly shifting cycle raster - elements wrap around and move up
    %one row as they are shifted circularly - this approach is much faster
    %than actually regneerating the cycle raster
    RASTERc(:,k)=RASTERt(:,k);

    %Shifting the Mask
    Mask=circshift(Mask',k)';
    
    %Generating coded words at a fixed phase
    Wt(k,:)=RASTERc*Mask';

    %Generating Word Histogram
    W=Wt(k,:);
    Nt=[];
    clear N
    while length(W)>=1
        Nt=[Nt sum(W(1)==W)];
        index=find(W(1)~=W);
        W=W(index);
    end
    PPt=sort(Nt/sum(Nt));
    Pt(k).Pt=PPt;
    
    %Computing Noise Entropy
    HWordt(k)=sum(PPt.*log2(1./PPt));

%     %%%%%%%%%%%%%% SHUFFLED NOISE DISTRIBUTION %%%%%%%%%%%%%%%%%%%%%
%     
%     %Computing Shuffled Noise Distribution
%     RASTERsh=rastershufflepanzeri(RASTERc);
% %     for l=1:500    %used to test independent distribution 
% %          RASTERsh=[RASTERsh;rastershufflepanzeri(RASTERc)];
% %     end
%     Wsh=RASTERsh*Mask';
%     
%     %Generating Shuffled Word Histogram
%     Ntsh=[];
%     while length(Wsh)>=1
%         Ntsh=[Ntsh sum(Wsh(1)==Wsh)];
%         index=find(Wsh(1)~=Wsh);
%         Wsh=Wsh(index);
%     end
%     PPs=sort(Ntsh/sum(Ntsh));
%     Pt(k).Psht=PPs;
%     
%     %Computing Shuffled Noise Entropy
%     HWordsht(k)=sum(PPs.*log2(1./PPs));
%       
%     %%%%%%%%%%%% INDEPENDENT NOISE DISTRIBUTION %%%%%%%%%%%%%%%%%%%
%     RASTERci=full(RASTERc);
%     Hit(k)=0;
%     for m=1:size(RASTERc,2)
%         
%         Pi=[];
%         for L=0:D
%            Pi(L+1)=length(find(RASTERci(:,m)==L))/size(RASTERci,1);  
%         end
%         index=find(Pi~=0);
%         Hit(k)=Hit(k)-sum(Pi(index).*log2(Pi(index)));
%     
%     end
%     
%    
% 




%     %Computing Independent Word Distribution (Panzeri et al) for words that
%     %are present in RASTERc & RASTERsh
%     %RASi=[RASTERc;RASTERsh];
% %   RASi=[RASTERc];
% %   RASi=[RASTERc];
%     RASi=[];
% %     for l=1:1
%          RASi=[RASi;rastershufflepanzeri(RASTERc)];
% %     end
% 
%     N1=size(RASTERc,1);
%     N2=size(RASTERc,2);
%     for l=1:B
%         
%         Wi=RASTERc(:,l);
%         Pind(l).Pi=[];
%         Pind(l).Wi=[];
%         while length(Wi)>0   
%             
%             %Finding independent probabilities for each bin
%             N=sum(Wi(1)==Wi);
%             Pind(l).Pi=[Pind(l).Pi N/N1];
%             Pind(l).Wi=[Pind(l).Wi Wi(1)];
%             index=find(Wi~=Wi(1));
%             Wi=Wi(index);
%             
%         end
%     end
% 
%     %Finding Pind for Non Repeated Words
%    
%     Wi=RASi*Mask';
%     [Wi,index]=sort(Wi);
%     i=find(diff(log2(Wi+1E-6)));
%     RASi=RASi(index(i),:);
%     PPi=zeros(size(RASi));
%     Wi=Wi(i);
%     for l=1:B
%         for m=1:length(Pind(l).Wi)
%             ii=find(Pind(l).Wi(m)==RASi(:,l));
%             PPi(ii,l)=Pind(l).Pi(m)*ones(size(ii));
%         end
%     end
% 
%     %PPi=PPi(index(i),:);
%     
% 
%     Pi=prod(PPi,2);
% %    i=find(Pi~=0);
% %    PPi=PPi(i,:);
%     
% %imagesc(PPi)
% %    pause
% %    Pi=Pi(i);
%     Pi=Pi/sum(Pi);
% 
%    
% %    Pi=prod(PPi,2);
% %    Pi=Pi(index(i));
%     %k
%     %sum(Pi)
%     
%     %Pi=Pi/sum(Pi);
%    %semilogy(sort(Pi),'ro')
%    %pause(0.1)
%     
%    
% 
% 
% 
%     
%   
% %     %Finding Word Probabilities Assuming independent bins
% %     for l=1:B
% %         for m=1:length(Pind(l).Wi)
% %             index=find(Pind(l).Wi(m)==RASi(:,l));
% %             PPi(index,l)=Pind(l).Pi(m)*ones(size(index));
% %         end
% %     end
% %     Pi=prod(PPi,2);
% %    
% %     %Finding only those elements within RASTERc and RASTERsh - removing
% %     %all repeated words
% % 
% %     
% %     
% %     Wi=RASi*Mask';
% %     [Wi,index]=sort(Wi);
% %     Pi=Pi(index);
% %     i=find(diff(log2(Wi+1E-6)));
% %     Pi=Pi(i);           %Find the probabilities only for those Words found in RASTERc and RASTERsh
% %     %Pi=Pi/sum(Pi);      %Normalize distribtion for unit area
% 
% 
% 
%     %Computing Independent Noise Entropy
%     HWordindt(k)=sum(Pi.*log2(1./Pi));
% 
% 
%     
% %     semilogy(PPt)
% %     hold on
% %     semilogy(sort(PPs),'g')
% %     semilogy(sort(Pi),'r')
% %     pause

end
HWordt=mean(HWordt);
%HWordsht=mean(HWordsht);
%HWordindt=mean(Hit);

%Finding Total Entropy
W=reshape(Wt,1,numel(Wt));
N=hist(log10(W+1),10000);
P=sort(N/sum(N));
index=find(P~=0);
HWord=sum(P(index).*log2(1./P(index)));

%Mean Spike Rate
Rate=numel([RASTER.spet])/RASTER(1).T/length(RASTER);

%Entropy per time and per spike
InfoData.HWordt=HWordt;
%InfoData.HWordsht=HWordsht;
%InfoData.HWordindt=HWordindt;
InfoData.HSect=HWordt/dt/B;
%InfoData.HSecsht=HWordsht/dt/B;
%InfoData.HSecindt=HWordindt/dt/B;
InfoData.HSpiket=InfoData.HSect/Rate;
%InfoData.HSpikesht=InfoData.HSecsht/Rate;
%InfoData.HSpikeindt=InfoData.HSecindt/Rate;
InfoData.HWord=HWord;
InfoData.HSec=HWord/dt/B;
InfoData.HSpike=InfoData.HSec/Rate;
InfoData.Rate=Rate;
InfoData.W=W;
InfoData.Wt=Wt;
InfoData.P=P;
InfoData.Pt=Pt;
InfoData.dt=dt;
InfoData.Fm=Fm;